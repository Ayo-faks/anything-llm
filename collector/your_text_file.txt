
Search in video
0:00
GWENDOLYN STRIPLING: Hello.
0:01
And welcome to Introduction to Generative AI.
0:04
My name is Dr. Gwendolyn Stripling.
0:06
And I am the artificial intelligence
0:09
technical curriculum developer here at Google Cloud.
0:14
In this course, you learn to define generative AI,
0:18
explain how generative AI works, describe generative AI model
0:23
types, and describe generative AI applications.
0:28
Generative AI is a type of artificial intelligence
0:31
technology that can produce various types of content,
0:36
including text, imagery, audio, and synthetic data.
0:41
But what is artificial intelligence?
0:44
Well, since we are going to explore
0:46
generative artificial intelligence,
0:48
let's provide a bit of context.
0:51
So two very common questions asked
0:53
are what is artificial intelligence
0:55
and what is the difference between AI and machine
1:00
learning.
1:01
One way to think about it is that AI is a discipline,
1:05
like physics for example.
1:08
AI is a branch of computer science
1:11
that deals with the creation of intelligence agents, which
1:15
are systems that can reason, and learn, and act autonomously.
1:20
Essentially, AI has to do with the theory and methods
1:24
to build machines that think and act like humans.
1:30
In this discipline, we have machine learning,
1:33
which is a subfield of AI.
1:35
It is a program or system that trains a model from input data.
1:40
That trained model can make useful predictions
1:42
from new or never before seen data
1:45
drawn from the same one used to train the model.
1:49
Machine learning gives the computer
1:51
the ability to learn without explicit programming.
1:56
Two of the most common classes of machine learning models
1:59
are unsupervised and supervised ML models.
2:03
The key difference between the two
2:05
is that, with supervised models, we have labels.
2:09
Labeled data is data that comes with a tag like a name, a type,
2:14
or a number.
2:16
Unlabeled data is data that comes with no tag.
2:20
This graph is an example of the problem
2:23
that a supervised model might try to solve.
2:26
For example, let's say you are the owner of a restaurant.
2:29
You have historical data of the bill amount
2:32
and how much different people tipped based on order type
2:36
and whether it was picked up or delivered.
2:39
In supervised learning, the model learns from past examples
2:42
to predict future values, in this case tips.
2:47
So here the model uses the total bill amount
2:49
to predict the future tip amount based on whether an order was
2:54
picked up or delivered.
2:57
This is an example of the problem
2:58
that an unsupervised model might try to solve.
3:02
So here you want to look at tenure and income
3:05
and then group or cluster employees
3:08
to see whether someone is on the fast track.
3:11
Unsupervised problems are all about discovery,
3:14
about looking at the raw data and seeing if it naturally
3:18
falls into groups.
3:21
Let's get a little deeper and show this graphically
3:24
as understanding these concepts are
3:27
the foundation for your understanding of generative AI.
3:31
In supervised learning, testing data values or x
3:35
are input into the model.
3:37
The model outputs a prediction and compares that prediction
3:42
to the training data used to train the model.
3:45
If the predicted test data values and actual training data
3:50
values are far apart, that's called error.
3:54
And the model tries to reduce this error
3:56
until the predicted and actual values are closer together.
4:01
This is a classic optimization problem.
4:05
Now that we've explored the difference
4:07
between artificial intelligence and machine learning,
4:10
and supervised and unsupervised learning,
4:13
let's briefly explore where deep learning
4:16
fits as a subset of machine learning methods.
4:20
While machine learning is a broad field that
4:22
encompasses many different techniques,
4:25
deep learning is a type of machine learning
4:27
that uses artificial neural networks,
4:29
allowing them to process more complex patterns than machine
4:32
learning.
4:34
Artificial neural networks are inspired by the human brain.
4:37
They are made up of many interconnected nodes or neurons
4:41
that can learn to perform tasks by processing data and making
4:46
predictions.
4:47
Deep learning models typically have many layers
4:49
of neurons, which allows them to learn
4:52
more complex patterns than traditional machine learning
4:55
models.
4:56
And neural networks can use both labeled and unlabeled data.
5:00
This is called semi-supervised learning.
5:03
In semi-supervised learning, a neural network
5:06
is trained on a small amount of labeled data
5:09
and a large amount of unlabeled data.
5:12
The labeled data helps the neural network
5:15
to learn the basic concepts of the task
5:17
while the unlabeled data helps the neural network
5:20
to generalize to new examples.
5:24
Now we finally get to where generative AI
5:27
fits into this AI discipline.
5:30
Gen AI is a subset of deep learning, which
5:33
means it uses artificial neural networks,
5:36
can process both labeled and unlabeled data using
5:40
supervised, unsupervised, and semi-supervised methods.
5:45
Large language models are also a subset of deep learning.
5:51
Deep learning models, or machine learning models in general,
5:54
can be divided into two types, generative and discriminative.
5:59
A discriminative model is a type of model
6:02
that is used to classify or predict labels for data points.
6:06
Discriminative models are typically
6:08
trained on a data set of labeled data points.
6:10
And they learn the relationship between the features
6:14
of the data points and the labels.
6:17
Once a discriminative model is trained,
6:20
it can be used to predict the label for new data points.
6:25
A generative model generates new data instances
6:28
based on a learned probability distribution of existing data.
6:33
Thus generative models generate new content.
6:38
Take this example here.
6:40
The discriminative model learns the conditional probability
6:42
distribution or the probability of y,
6:45
our output, given x, our input, that this is a dog
6:50
and classifies it as a dog and not a cat.
6:54
The generative model learns the joint probability distribution
6:58
or the probability of x and y and predicts
7:02
the conditional probability that this is a dog
7:05
and can then generate a picture of a dog.
7:09
So to summarize, generative models
7:11
can generate new data instances while discriminative models
7:16
discriminate between different kinds of data instances.
7:21
The top image shows a traditional machine
7:23
learning model which attempts to learn
7:25
the relationship between the data and the label,
7:28
or what you want to predict.
7:30
The bottom image shows a generative AI model
7:33
which attempts to learn patterns on content so that it
7:36
can generate new content.
7:40
A good way to distinguish what is gen AI and what is not
7:43
is shown in this illustration.
7:46
It is not gen AI when the output, or y, or label is
7:51
a number or a class, for example spam or not spam,
7:55
or a probability.
7:57
It is gen AI when the output is natural language, like speech
8:03
or text, an image or audio, for example.
8:08
Visualizing this mathematically would look like this.
8:12
If you haven't seen this for a while,
8:14
the y is equal to f of x equation calculates
8:18
the dependent output of a process given different inputs.
8:23
The y stands for the model output.
8:25
The f embodies the function used in the calculation.
8:29
And the x represents the input or inputs used for the formula.
8:33
So the model output is a function of all the inputs.
8:36
If the y is the number, like predicted sales,
8:41
it is not gen AI.
8:43
If y is a sentence, like define sales,
8:46
it is generative as the question would elicit a text response.
8:51
The response would be based on all the massive large data
8:55
the model was already trained on.
8:59
To summarize at a high level, the traditional, classical
9:03
supervised and unsupervised learning process
9:06
takes training code and label data to build a model.
9:09
Depending on the use case or problem,
9:12
the model can give you a prediction.
9:15
It can classify something or cluster something.
9:18
We use this example to show you how much more robust
9:22
the gen AI process is.
9:25
The gen AI process can take training code, label data,
9:29
and unlabeled data of all data types
9:31
and build a foundation model.
9:33
The foundation model can then generate new content.
9:36
For example, text, code, images, audio, video, et cetera.
9:42
We've come a long away from traditional programming
9:45
to neural networks to generative models.
9:48
In traditional programming, we used
9:50
to have to hard code the rules for distinguishing a cat--
9:53
the type, animal; legs, four; ears, two; fur, yes;
10:00
likes yarn and catnip.
10:03
In the wave of neural networks, we
10:05
could give the network pictures of cats and dogs
10:07
and ask is this a cat and it would predict a cat.
10:12
In the generative wave, we as users
10:15
can generate our own content, whether it
10:18
be text, images, audio, video, et cetera, for example
10:23
models like PaLM or Pathways Language Model,
10:26
or LAMBDA, Language Model for Dialogue Applications,
10:30
ingest very, very large data from the multiple sources
10:33
across the internet and build foundation language
10:36
models we can use simply by asking a question,
10:40
whether typing it into a prompt or verbally
10:43
talking into the prompt itself.
10:45
So when you ask it what's a cat, it
10:48
can give you everything it has learned about a cat.
10:52
Now we come to our formal definition.
10:55
What is generative AI?
10:57
Gen AI is a type of artificial intelligence
11:00
that creates new content based on what it has
11:02
learned from existing content.
11:05
The process of learning from existing content
11:07
is called training and results in the creation
11:10
of a statistical model when given a prompt.
11:13
AI uses the model to predict what an expected response might
11:18
be and this generates new content.
11:21
Essentially, it learns the underlying structure
11:24
of the data and can then generate
11:26
new samples that are similar to the data it was trained on.
11:31
As previously mentioned, a generative language model
11:35
can take what it has learned from the examples it's
11:38
been shown and create something entirely new
11:41
based on that information.
11:43
Large language models are one type of generative AI
11:47
since they generate novel combinations of text
11:52
in the form of natural sounding language.
11:56
A generative image model takes an image
11:59
as input and can output text, another image, or video.
12:04
For example, under the output text,
12:07
you can get visual question answering
12:09
while under output image, an image completion is generated.
12:14
And under output video, animation is generated.
12:19
A generative language model takes text as input
12:22
and can output more text, an image, audio, or decisions.
12:27
For example, under the output text,
12:29
question answering is generated.
12:31
And under output image, a video is generated.
12:35
We've stated that generative language models learn
12:38
about patterns and language through training data,
12:41
then, given some text, they predict what comes next.
12:46
Thus generative language models are pattern matching systems.
12:50
They learn about patterns based on the data you provide.
12:54
Here is an example.
12:57
Based on things it's learned from its training data,
12:59
it offers predictions of how to complete this sentence,
13:03
I'm making a sandwich with peanut butter and jelly.
13:09
Here is the same example using Bard,
13:12
which is trained on a massive amount of text data
13:15
and is able to communicate and generate
13:17
humanlike text in response to a wide range of prompts
13:21
and questions.
13:23
Here is another example.
13:25
The meaning of life is--
13:29
and Bart gives you a contextual answer
13:32
and then shows the highest probability response.
13:35
The power of generative AI comes from the use of transformers.
13:40
Transformers produced a 2018 revolution
13:43
in natural language processing.
13:45
At a high level, a transformer model
13:47
consists of an encoder and decoder.
13:50
The encoder encodes the input sequence
13:53
and passes it to the decoder, which
13:55
learns how to decode the representation
13:58
for a relevant task.
14:01
In transformers, hallucinations are words or phrases
14:06
that are generated by the model that
14:09
are often nonsensical or grammatically incorrect.
14:13
Hallucinations can be caused by a number of factors,
14:17
including the model is not trained on enough data,
14:21
or the model is trained on noisy or dirty data,
14:25
or the model is not given enough context,
14:29
or the model is not given enough constraints.
14:33
Hallucinations can be a problem for transformers
14:35
because they can make the output text difficult to understand.
14:40
They can also make the model more
14:41
likely to generate incorrect or misleading information.
14:46
A prompt is a short piece of text
14:49
that is given to the large language model as input.
14:53
And it can be used to control the output of the model
14:57
in a variety of ways.
14:59
Prompt design is the process of creating
15:01
a prompt that will generate the desired output
15:04
from a large language model.
15:07
As previously mentioned, gen AI depends a lot
15:11
on the training data that you have fed into it.
15:14
And it analyzes the patterns and structures of the input data
15:18
and thus learns.
15:20
But with access to a browser based prompt, you, the user,
15:23
can generate your own content.
15:27
We've shown illustrations of the types of input based upon data.
15:31
Here are the associated model types.
15:33
Text-to-text.
15:35
Text-to-text models take a natural language input
15:38
and produces a text output.
15:40
These models are trained to learn the mapping
15:43
between a pair of text, e.g.
15:45
for example, translation from one language to another.
15:49
Text-to-image.
15:50
Text-to-image models are trained on a large set of images,
15:54
each captioned with a short text description.
15:58
Diffusion is one method used to achieve this.
16:01
Text-to-video and text-to-3D.
16:04
Text-to-video models aim to generate a video representation
16:08
from text input.
16:09
The input text can be anything from a single sentence
16:13
to a full script.
16:15
And the output is a video that corresponds to the input text.
16:20
Similarly, text-to-3D models generate
16:23
three dimensional objects that correspond to a user's text
16:28
description.
16:29
For example, this can be used in games or other 3D worlds.
16:34
Text-to-task.
16:36
Text-to-task models are trained to perform a defined task
16:41
or action based on text input.
16:44
This task can be a wide range of actions
16:46
such as answering a question, performing a search,
16:50
making a prediction, or taking some sort of action.
16:55
For example, a text-to-task model
16:58
could be trained to navigate a web UI or make changes to a doc
17:03
through the GUI.
17:05
A foundation model is a large AI model pre-trained
17:08
on a vast quantity of data designed to be adapted or fine
17:13
tuned to a wide range of downstream tasks,
17:17
such as sentiment analysis, image captioning, and object
17:22
recognition.
17:23
Foundation models have the potential
17:26
to revolutionize many industries, including
17:29
health care, finance, and customer service.
17:32
They can be used to detect fraud and provide
17:36
personalized customer support.
17:38
Vertex AI offers a model garden that
17:41
includes foundation models.
17:43
The language foundation models include
17:45
PaLM API for chat and text.
17:48
The vision foundation models includes stable diffusion,
17:52
which has been shown to be effective at generating
17:55
high quality images from text descriptions.
18:00
Let's say you have a use case where
18:01
you need to gather sentiments about how your customers are
18:05
feeling about your product or service.
18:07
You can use the classification task sentiment analysis task
18:12
model for just that purpose.
18:15
And what if you needed to perform occupancy analytics?
18:19
There is a task model for your use case.
18:23
Shown here are gen AI applications.
18:27
Let's look at an example of code generation
18:30
shown in the second block under code at the top.
18:34
In this example, I've input a code file conversion problem,
18:39
converting from Python to JSON.
18:41
I use Bard.
18:42
And I insert into the prompt box the following.
18:46
I have a Pandas DataFrame with two columns, one with the file
18:50
name and one with the hour in which it is generated.
18:54
I'm trying to convert this into a JSON file
18:57
in the format shown onscreen.
19:00
Bard returns the steps I need to do this and the code snippet.
19:06
And here my output is in a JSON format.
19:10
It gets better.
19:11
I happen to be using Google's free, browser-based Jupyter
19:16
Notebook, known as Colab.
19:18
And I simply export the Python code to Google's Colab.
19:23
To summarize, Bart code generation
19:26
can help you debug your lines of source code,
19:29
explain your code to you line by line,
19:31
craft SQL queries for your database,
19:34
translate code from one language to another,
19:37
and generate documentation and tutorials for source code.
19:42
Generative AI Studio lets you quickly explore and customize
19:47
gen AI models that you can leverage in your applications
19:51
on Google Cloud.
19:53
Generative AI Studio helps developers create and deploy
19:57
Gen AI models by providing a variety of tools and resources
20:02
that make it easy to get started.
20:05
For example, there's a library of pre-trained models.
20:09
There is a tool for fine tuning models.
20:12
There is a tool for deploying models to production.
20:15
And there is a community forum for developers
20:18
to share ideas and collaborate.
20:21
Generative AI App Builder lets you
20:24
create gen AI apps without having to write any code.
20:28
Gen AI App Builder has a drag and drop interface
20:31
that makes it easy to design and build apps.
20:35
It has a visual editor that makes
20:36
it easy to create and edit app content.
20:39
It has a built-in search engine that
20:40
allows users to search for information within the app.
20:43
And it has a conversational AI Engine
20:46
that helps users to interact with the app using
20:49
natural language.
20:51
You can create your own digital assistants, custom search
20:55
engines, knowledge bases, training applications,
20:59
and much more.
21:01
PaLM API lets you test and experiment
21:05
with Google's large language models and gen AI tools.
21:09
To make prototyping quick and more accessible,
21:11
developers can integrate PaLM API with Maker suite
21:15
and use it to access the API using a graphical user
21:20
interface.
21:21
The suite includes a number of different tools such as a model
21:25
training tool, a model deployment tool, and a model
21:29
monitoring tool.
21:31
The model training tool helps developers train ML models
21:35
on their data using different algorithms.
21:38
The model deployment tool helps developers deploy ML models
21:42
to production with a number of different deployment options.
21:48
The model monitoring tool helps developers
21:51
monitor the performance of their ML models
21:54
in production using a dashboard and a number
21:58
of different metrics.
22:01
Thank you for watching our course, Introduction
22:04
to Generative AI.

